{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Detection Model\n",
    "\n",
    "This is the Audio Detection Model which have been trained individually to recognize and classify audio files which have been previously saved as spectrograms in a separate directory. Weights have been saved as pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21224,
     "status": "ok",
     "timestamp": 1608447828178,
     "user": {
      "displayName": "winter project",
      "photoUrl": "",
      "userId": "09807745845121370616"
     },
     "user_tz": -330
    },
    "id": "RzkqynSkEdwg",
    "outputId": "db9340be-2104-4dce-e7c8-68b8339ae61b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "#drive.mount(\"/content/gdrive\", force_remount=True)\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FBoGwFtEkOv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import wave\n",
    "import pylab\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Set paths to input and output data\n",
    "#INPUT_DIR = '/content/gdrive/MyDrive/data/vox1_test_wav/wav/id10270/5r0dWxy17C8'\n",
    "OUTPUT_DIR = '/content/gdrive/MyDrive/t3'\n",
    "\n",
    "# Print names of 10 WAV files from the input path\n",
    "# parent_list = os.listdir(INPUT_DIR)\n",
    "# for i in range(10):\n",
    "#     print(parent_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2554,
     "status": "ok",
     "timestamp": 1608391081745,
     "user": {
      "displayName": "winter project",
      "photoUrl": "",
      "userId": "09807745845121370616"
     },
     "user_tz": -330
    },
    "id": "T-gCZVpQEkLU",
    "outputId": "305b5b6c-1e5a-4912-97d4-f96b1c10efab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4883 files belonging to 40 classes.\n",
      "Using 3907 files for training.\n",
      "Found 4883 files belonging to 40 classes.\n",
      "Using 976 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Declare constants\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_WIDTH = 256\n",
    "BATCH_SIZE = 64\n",
    "N_CHANNELS = 3\n",
    "N_CLASSES = 40\n",
    "\n",
    "# Make a dataset containing the training spectrograms\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             directory=os.path.join(OUTPUT_DIR, 'audio-images'),\n",
    "                                             shuffle=True,\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                             subset=\"training\",\n",
    "                                             seed=0)\n",
    "\n",
    "# Make a dataset containing the validation spectrogram\n",
    "valid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             directory=os.path.join(OUTPUT_DIR, 'audio-images'),\n",
    "                                             shuffle=True,\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                             subset=\"validation\",\n",
    "                                             seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Il3d1y4OEkJq"
   },
   "outputs": [],
   "source": [
    "# Function to prepare our datasets for modelling\n",
    "def prepare(ds, augment=False):\n",
    "    # Define our one transformation\n",
    "    rescale = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.Rescaling(1./255)])\n",
    "    flip_and_rotate = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n",
    "    ])\n",
    "    \n",
    "    #Apply rescale to both datasets and augmentation only to training\n",
    "    ds = ds.map(lambda x, y: (rescale(x, training=True), y))\n",
    "    if augment: \n",
    "      ds = ds.map(lambda x, y: (flip_and_rotate(x, training=True), y))\n",
    "    return ds\n",
    "\n",
    "train_dataset = prepare(train_dataset, augment=False)\n",
    "valid_dataset = prepare(valid_dataset, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOw-x9czEkGo"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create CNN model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\n",
    "model.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "checkpoint_filepath = '/content/gdrive/MyDrive/tmp/checkpoint/trying_archi/weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "# Train model for 10 epochs, capture the history\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=valid_dataset, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y959MV07htBy"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create CNN model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\n",
    "model.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.005))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.01))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.05))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "# checkpoint_path='/content/gdrive/MyDrive/tmp/checkpoint/trying_archi/weights.08-2.08.hdf5'\n",
    "# model.load_weights(checkpoint_path)\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint_filepath = '/content/gdrive/MyDrive/tmp/checkpoint/trying_archi/from start/weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "# Train model for 10 epochs, capture the history\n",
    "history = model.fit(train_dataset, epochs=100, validation_data=valid_dataset, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDktdLtW44DG"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create CNN model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\n",
    "model.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.005))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.01))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.05))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "checkpoint_path='/content/gdrive/MyDrive/tmp/checkpoint/op weights a/weights.42-1.05.hdf5'\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam',\n",
    "    ),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint_filepath = '/content/gdrive/MyDrive/tmp/checkpoint/trying_archi/after 42 adam/second run/weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "# Train model for 10 epochs, capture the history\n",
    "history = model.fit(train_dataset, epochs=8, validation_data=valid_dataset, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1231403,
     "status": "ok",
     "timestamp": 1608392316707,
     "user": {
      "displayName": "winter project",
      "photoUrl": "",
      "userId": "09807745845121370616"
     },
     "user_tz": -330
    },
    "id": "mz1t77I_JE0m",
    "outputId": "25c2224f-5612-4c46-fd01-6a3d4761ca25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 1229s 20s/step - loss: 0.7229 - accuracy: 0.7610 - val_loss: 1.0143 - val_accuracy: 0.7346\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create CNN model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\n",
    "model.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.005))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.01))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.05))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "checkpoint_path='/content/gdrive/MyDrive/tmp/checkpoint/op weights a/weights.42-1.05.hdf5'\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam',\n",
    "    ),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint_filepath = '/content/gdrive/MyDrive/tmp/checkpoint/trying_archi/after 42 adam/second run/weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "# Train model for 10 epochs, capture the history\n",
    "history = model.fit(train_dataset, epochs=1, validation_data=valid_dataset, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24311,
     "status": "ok",
     "timestamp": 1608393291116,
     "user": {
      "displayName": "winter project",
      "photoUrl": "",
      "userId": "09807745845121370616"
     },
     "user_tz": -330
    },
    "id": "i1rouVMOKkQo",
    "outputId": "8434a25f-f892-436c-e9b8-e9fc5349b702"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 22s 311ms/step - loss: 0.1582 - accuracy: 0.9759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15817782282829285, 0.9759406447410583]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28434,
     "status": "ok",
     "timestamp": 1608393296599,
     "user": {
      "displayName": "winter project",
      "photoUrl": "",
      "userId": "09807745845121370616"
     },
     "user_tz": -330
    },
    "id": "9JTdvi1bJcwR",
    "outputId": "1d6f8e13-cbf1-404e-d085-11db46e9a8ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 5s 174ms/step - loss: 1.0143 - accuracy: 0.7346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.014265775680542, 0.7346311211585999]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33329,
     "status": "ok",
     "timestamp": 1608393302040,
     "user": {
      "displayName": "winter project",
      "photoUrl": "",
      "userId": "09807745845121370616"
     },
     "user_tz": -330
    },
    "id": "IEmj-iT7EkFI",
    "outputId": "87bc99f5-3734-4078-fb91-bfef45127e3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 1.014266, final accuracy: 0.734631\n"
     ]
    }
   ],
   "source": [
    "final_loss, final_acc = model.evaluate(valid_dataset, verbose=0)\n",
    "print(\"Final loss: {0:.6f}, final accuracy: {1:.6f}\".format(final_loss, final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22240,
     "status": "ok",
     "timestamp": 1608394180619,
     "user": {
      "displayName": "winter project",
      "photoUrl": "",
      "userId": "09807745845121370616"
     },
     "user_tz": -330
    },
    "id": "rzt1ohVowkJw",
    "outputId": "39855f2a-bff7-409a-923b-a7d4bb23314b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 22s 306ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.58411412e-10, 9.03755063e-05, 8.42600421e-04, ...,\n",
       "        2.07468224e-06, 1.18364629e-09, 1.76585206e-06],\n",
       "       [1.77458950e-20, 8.60816414e-08, 8.65279029e-12, ...,\n",
       "        5.80467510e-17, 2.80201267e-26, 1.07496634e-16],\n",
       "       [1.12748451e-08, 9.06327280e-09, 7.66616270e-10, ...,\n",
       "        4.19729460e-08, 2.77601431e-09, 3.02615621e-09],\n",
       "       ...,\n",
       "       [3.56871234e-11, 1.42721092e-05, 5.95565064e-09, ...,\n",
       "        6.35487702e-13, 2.61182338e-14, 9.43228588e-05],\n",
       "       [2.02083399e-13, 1.57924787e-05, 3.40364932e-05, ...,\n",
       "        3.60477065e-11, 3.87222138e-17, 5.33863204e-04],\n",
       "       [1.04245612e-24, 1.10753418e-09, 9.06865506e-20, ...,\n",
       "        3.24195578e-25, 1.47011198e-29, 6.54804029e-15]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(train_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLjB2-D7tZ0l",
    "outputId": "ddcca5c9-5f29-4f66-f06b-1a9724570169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "62/62 [==============================] - 30s 444ms/step - loss: 0.6831 - accuracy: 0.7840 - val_loss: 0.9963 - val_accuracy: 0.7418\n",
      "Epoch 2/5\n",
      "62/62 [==============================] - 30s 445ms/step - loss: 0.6759 - accuracy: 0.7801 - val_loss: 0.9823 - val_accuracy: 0.7428\n",
      "Epoch 3/5\n",
      "62/62 [==============================] - 30s 449ms/step - loss: 0.6626 - accuracy: 0.7919 - val_loss: 0.9782 - val_accuracy: 0.7469\n",
      "Epoch 4/5\n",
      "62/62 [==============================] - 31s 462ms/step - loss: 0.6835 - accuracy: 0.7789 - val_loss: 0.9694 - val_accuracy: 0.7490\n",
      "Epoch 5/5\n",
      "62/62 [==============================] - 31s 453ms/step - loss: 0.6827 - accuracy: 0.7814 - val_loss: 0.9689 - val_accuracy: 0.7510\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=5, validation_data=valid_dataset, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QM6R5xLhxBlr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNiSTHTywBP7ZHH0k0GAPDe",
   "collapsed_sections": [],
   "name": "trying_archi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
