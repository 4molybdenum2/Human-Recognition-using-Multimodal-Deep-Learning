{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mutlimodal Human Verfication.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "minMWXytkKEb"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "import cv2\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import datasets, layers, models\r\n",
        "from keras.layers import Add, Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D , Dropout\r\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\r\n",
        "from keras.models import Model\r\n",
        "from keras.losses import categorical_crossentropy\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.regularizers import l2\r\n",
        "from keras.utils import np_utils\r\n",
        "\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from keras.initializers import glorot_uniform\r\n",
        "\r\n",
        "from keras.applications.vgg16 import VGG16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_nkvYc_bMgX",
        "outputId": "5fab99d2-6ecf-484e-903e-5344b315ad38"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUWqwdMTfjk-"
      },
      "source": [
        "# Declare constants\r\n",
        "IMAGE_HEIGHT = 128\r\n",
        "IMAGE_WIDTH = 128\r\n",
        "BATCH_SIZE = 64\r\n",
        "N_CHANNELS = 3\r\n",
        "N_CLASSES = 20\r\n",
        "IMG_SHAPE = (IMAGE_HEIGHT,IMAGE_WIDTH,N_CHANNELS)\r\n",
        "\r\n",
        "AUD_HEIGHT = 128\r\n",
        "AUD_WIDTH = 128\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3ZJUZfPcOUj",
        "outputId": "de84dc04-7f00-4e62-b603-0cf5e8c33902"
      },
      "source": [
        "# Creating training data\r\n",
        "# dont run this\r\n",
        "main_dir = '/content/gdrive/My Drive'\r\n",
        "data_dir = main_dir + '/faces20/'\r\n",
        "categories = os.listdir(main_dir + '/faces20')\r\n",
        "data = []\r\n",
        "categorical_index = []\r\n",
        "def create_face_data():\r\n",
        "  for category in categories:\r\n",
        "    path = os.path.join(data_dir+category)\r\n",
        "    class_num = categories.index(category)\r\n",
        "    categorical_index.append([category,class_num])\r\n",
        "    for img in tqdm(os.listdir(path)):\r\n",
        "      try:\r\n",
        "        img_array_bgr = cv2.imread(os.path.join(path,img))\r\n",
        "        img_array = cv2.cvtColor(img_array_bgr , cv2.COLOR_BGR2RGB)\r\n",
        "        new_array = cv2.resize(img_array , (IMAGE_HEIGHT,IMAGE_WIDTH))\r\n",
        "        data.append([new_array,class_num])\r\n",
        "      except Exception as e:\r\n",
        "        pass\r\n",
        "\r\n",
        "create_face_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 34/34 [00:13<00:00,  2.61it/s]\n",
            "100%|██████████| 30/30 [00:11<00:00,  2.65it/s]\n",
            "100%|██████████| 50/50 [00:18<00:00,  2.67it/s]\n",
            "100%|██████████| 25/25 [00:09<00:00,  2.67it/s]\n",
            "100%|██████████| 29/29 [00:10<00:00,  2.67it/s]\n",
            "100%|██████████| 37/37 [00:14<00:00,  2.64it/s]\n",
            "100%|██████████| 23/23 [00:08<00:00,  2.68it/s]\n",
            "100%|██████████| 63/63 [00:24<00:00,  2.55it/s]\n",
            "100%|██████████| 24/24 [00:09<00:00,  2.58it/s]\n",
            "100%|██████████| 27/27 [00:10<00:00,  2.48it/s]\n",
            "100%|██████████| 25/25 [00:07<00:00,  3.40it/s]\n",
            "100%|██████████| 24/24 [00:09<00:00,  2.61it/s]\n",
            "100%|██████████| 35/35 [00:13<00:00,  2.61it/s]\n",
            "100%|██████████| 50/50 [00:18<00:00,  2.66it/s]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.60it/s]\n",
            "100%|██████████| 27/27 [00:10<00:00,  2.55it/s]\n",
            "100%|██████████| 31/31 [00:17<00:00,  1.77it/s]\n",
            "100%|██████████| 50/50 [00:20<00:00,  2.49it/s]\n",
            "100%|██████████| 24/24 [00:09<00:00,  2.56it/s]\n",
            "100%|██████████| 27/27 [00:10<00:00,  2.58it/s]\n",
            "100%|██████████| 47/47 [00:18<00:00,  2.59it/s]\n",
            "100%|██████████| 36/36 [00:13<00:00,  2.62it/s]\n",
            "100%|██████████| 36/36 [00:14<00:00,  2.57it/s]\n",
            "100%|██████████| 35/35 [00:13<00:00,  2.59it/s]\n",
            "100%|██████████| 61/61 [00:24<00:00,  2.45it/s]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.66it/s]\n",
            "100%|██████████| 53/53 [00:20<00:00,  2.61it/s]\n",
            "100%|██████████| 39/39 [00:15<00:00,  2.60it/s]\n",
            "100%|██████████| 32/32 [00:12<00:00,  2.61it/s]\n",
            "100%|██████████| 43/43 [00:09<00:00,  4.57it/s]\n",
            "100%|██████████| 25/25 [00:09<00:00,  2.65it/s]\n",
            "100%|██████████| 22/22 [00:08<00:00,  2.61it/s]\n",
            "100%|██████████| 35/35 [00:13<00:00,  2.60it/s]\n",
            "100%|██████████| 17/17 [00:06<00:00,  2.67it/s]\n",
            "100%|██████████| 39/39 [00:14<00:00,  2.61it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_IrC0SktxlO",
        "outputId": "4f36b88e-f665-40cf-dd5e-82390513eeee"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1191"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2n4P8Fy2EKj",
        "outputId": "8a49e504-65ab-47fd-c2a0-591f73149794"
      },
      "source": [
        "main_dir1 = '/content/gdrive/My Drive'\r\n",
        "data_dir1 = main_dir1 + '/t3/audio-images/'\r\n",
        "categories1 = os.listdir(main_dir1 + '/t3/audio-images')\r\n",
        "data1 = []\r\n",
        "categorical_index1 = []\r\n",
        "def create_audio_data():\r\n",
        "  for category in categories1:\r\n",
        "    path = os.path.join(data_dir1+category)\r\n",
        "    class_num = categories1.index(category)\r\n",
        "    categorical_index1.append([category,class_num])\r\n",
        "    for img in tqdm(os.listdir(path)):\r\n",
        "      try:\r\n",
        "        img_array_bgr = cv2.imread(os.path.join(path,img))\r\n",
        "        img_array = cv2.cvtColor(img_array_bgr , cv2.COLOR_BGR2RGB)\r\n",
        "        new_array = cv2.resize(img_array , (IMAGE_HEIGHT,IMAGE_WIDTH))\r\n",
        "        data1.append([new_array,class_num])\r\n",
        "      except Exception as e:\r\n",
        "        pass\r\n",
        "\r\n",
        "create_audio_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 166/166 [00:01<00:00, 127.17it/s]\n",
            "100%|██████████| 107/107 [00:00<00:00, 119.21it/s]\n",
            "100%|██████████| 162/162 [00:01<00:00, 127.87it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 127.36it/s]\n",
            "100%|██████████| 304/304 [00:02<00:00, 130.87it/s]\n",
            "100%|██████████| 194/194 [00:01<00:00, 125.64it/s]\n",
            "100%|██████████| 127/127 [00:01<00:00, 104.90it/s]\n",
            "100%|██████████| 265/265 [00:02<00:00, 98.65it/s]\n",
            "100%|██████████| 137/137 [00:01<00:00, 100.62it/s]\n",
            "100%|██████████| 138/138 [00:01<00:00, 97.02it/s]\n",
            "100%|██████████| 88/88 [00:00<00:00, 95.90it/s]\n",
            "100%|██████████| 79/79 [00:00<00:00, 99.51it/s] \n",
            "100%|██████████| 98/98 [00:00<00:00, 116.58it/s]\n",
            "100%|██████████| 49/49 [00:00<00:00, 128.70it/s]\n",
            "100%|██████████| 76/76 [00:00<00:00, 125.68it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 132.21it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 126.90it/s]\n",
            "100%|██████████| 84/84 [00:00<00:00, 123.40it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 129.72it/s]\n",
            "100%|██████████| 90/90 [00:00<00:00, 120.11it/s]\n",
            "100%|██████████| 93/93 [00:00<00:00, 116.01it/s]\n",
            "100%|██████████| 149/149 [00:01<00:00, 124.20it/s]\n",
            "100%|██████████| 84/84 [00:00<00:00, 125.18it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 125.83it/s]\n",
            "100%|██████████| 233/233 [00:01<00:00, 131.82it/s]\n",
            "100%|██████████| 63/63 [00:00<00:00, 125.11it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 123.73it/s]\n",
            "100%|██████████| 54/54 [00:00<00:00, 128.85it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 126.06it/s]\n",
            "100%|██████████| 185/185 [00:01<00:00, 130.88it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 127.65it/s]\n",
            "100%|██████████| 73/73 [00:00<00:00, 126.86it/s]\n",
            "100%|██████████| 187/187 [00:01<00:00, 129.24it/s]\n",
            "100%|██████████| 240/240 [00:01<00:00, 126.61it/s]\n",
            "100%|██████████| 163/163 [00:01<00:00, 127.04it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn3YWrOCCFz1",
        "outputId": "002621d6-de59-4a87-b18f-8aaeade3700b"
      },
      "source": [
        "len(data1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4177"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-1c9qRduSRq"
      },
      "source": [
        "# dont run this\r\n",
        "import random\r\n",
        "\r\n",
        "random.shuffle(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYlv1Owst0l8"
      },
      "source": [
        "# creating X and y from data list\r\n",
        "\r\n",
        "X = []\r\n",
        "y = []\r\n",
        "\r\n",
        "for features, labels in data:\r\n",
        "  X.append(features)\r\n",
        "  y.append(labels)\r\n",
        "\r\n",
        "# converting into numpy arrays\r\n",
        "X = np.array(X).reshape(-1,IMAGE_HEIGHT,IMAGE_WIDTH,3)\r\n",
        "y = np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCTXlPvp_Q7N"
      },
      "source": [
        "random.shuffle(data1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VVy8eCAB3cG"
      },
      "source": [
        "# creating X and y from data list\r\n",
        "\r\n",
        "X1 = []\r\n",
        "y1 = []\r\n",
        "\r\n",
        "for features, labels in data1:\r\n",
        "  X1.append(features)\r\n",
        "  y1.append(labels)\r\n",
        "\r\n",
        "# converting into numpy arrays\r\n",
        "X1 = np.array(X1).reshape(-1,IMAGE_HEIGHT,IMAGE_WIDTH,3)\r\n",
        "y1 = np.array(y1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeEpjm_6vwCs"
      },
      "source": [
        "# Save the created data using pickle so we don't have tp run the create_data() function again\r\n",
        "\r\n",
        "import pickle\r\n",
        "\r\n",
        "pickle_out = open('/content/gdrive/My Drive/data/X.pickle' , 'wb')\r\n",
        "pickle.dump(X , pickle_out)\r\n",
        "pickle_out.close()\r\n",
        "\r\n",
        "pickle_out = open('/content/gdrive/My Drive/data/y.pickle' , 'wb')\r\n",
        "pickle.dump(y , pickle_out)\r\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03Gr5jz-DfDl"
      },
      "source": [
        "import pickle\r\n",
        "\r\n",
        "pickle_out = open('/content/gdrive/My Drive/data/X1.pickle' , 'wb')\r\n",
        "pickle.dump(X1 , pickle_out)\r\n",
        "pickle_out.close()\r\n",
        "\r\n",
        "pickle_out = open('/content/gdrive/My Drive/data/y1.pickle' , 'wb')\r\n",
        "pickle.dump(y1 , pickle_out)\r\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3luFnRdiwLHg"
      },
      "source": [
        "# load the saved data\r\n",
        "\r\n",
        "# start running from here\r\n",
        "\r\n",
        "pickle_in = open('/content/gdrive/My Drive/data/X.pickle' , 'rb')\r\n",
        "X = pickle.load(pickle_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q1pYndgDwMV"
      },
      "source": [
        "# load audio saved data\r\n",
        "\r\n",
        "pickle_in = open('/content/gdrive/My Drive/data/X1.pickle' , 'rb')\r\n",
        "X1 = pickle.load(pickle_in)\r\n",
        "\r\n",
        "pickle_in = open('/content/gdrive/My Drive/data/y1.pickle' , 'rb')\r\n",
        "y1 = pickle.load(pickle_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sa0ZAFjJQlo",
        "outputId": "e55463d2-9520-47da-fc7e-d6ce592ba5a7"
      },
      "source": [
        "# creating train test split data\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1)\r\n",
        "\r\n",
        "print('Shape of X_train:' + str(X_train.shape))\r\n",
        "print('Shape of y_train:' + str(y_train.shape))\r\n",
        "print('Shape of X_test:' + str(X_test.shape))\r\n",
        "print('Shape of y_test:' + str(y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train:(1071, 128, 128, 3)\n",
            "Shape of y_train:(1071,)\n",
            "Shape of X_test:(120, 128, 128, 3)\n",
            "Shape of y_test:(120,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-psfPkQD7uR",
        "outputId": "910c4745-3d3e-4e75-c1dd-1e68e87d3354"
      },
      "source": [
        "# creating train test split for audio data\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X1_train,X1_test,y1_train,y1_test = train_test_split(X1,y1,test_size=0.1)\r\n",
        "\r\n",
        "print('Shape of X_train:' + str(X1_train.shape))\r\n",
        "print('Shape of y_train:' + str(y1_train.shape))\r\n",
        "print('Shape of X_test:' + str(X1_test.shape))\r\n",
        "print('Shape of y_test:' + str(y1_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train:(3759, 128, 128, 3)\n",
            "Shape of y_train:(3759,)\n",
            "Shape of X_test:(418, 128, 128, 3)\n",
            "Shape of y_test:(418,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeikGCDQ433u"
      },
      "source": [
        "img_a = []\r\n",
        "img_b = []\r\n",
        "img_y = []\r\n",
        "\r\n",
        "def create_image_data_pairs():\r\n",
        "\r\n",
        "  # our objective is to make half of the dataset witht the same classes and half with different classes.\r\n",
        "\r\n",
        "  # same class\r\n",
        "  i = 0\r\n",
        "  while(i<2500):\r\n",
        "\r\n",
        "    # perform pairing for same classes.\r\n",
        "    same_class = np.random.randint(N_CLASSES) # 5\r\n",
        "\r\n",
        "    index = random.choice([k for k,j in enumerate(y) if j==same_class] , 2, replace=False)\r\n",
        "    x_random = X[index]\r\n",
        "    y_same = same_class\r\n",
        "\r\n",
        "    img_a.append(x_random[0])\r\n",
        "    img_b.append(x_random[1])\r\n",
        "\r\n",
        "    img_y.append(y_same)\r\n",
        "\r\n",
        "  # different classes\r\n",
        "  i = 0\r\n",
        "  while(i<2500):\r\n",
        "    # Repeat this as many times as many examples you want using a loop\r\n",
        "\r\n",
        "    # select random indices from the shape[0] of the X_train array\r\n",
        "    index = np.random.choice(X.shape[0], 2, replace=False)  \r\n",
        "\r\n",
        "    # make a random array by choosing subsets from X_train\r\n",
        "    x_random = X[index]\r\n",
        "    y_random = y[index]\r\n",
        "\r\n",
        "    # append the first image to img_a and second to img_b\r\n",
        "    img_a.append(x_random[0])\r\n",
        "    img_b.append(x_random[1])\r\n",
        "\r\n",
        "    # finally based on the similarity of the images based on their classes append either the class (if they are similar) or -1 if they are dissimilar\r\n",
        "    if(y_random[0] == y_random[1]):\r\n",
        "      pass\r\n",
        "    else:\r\n",
        "      img_y.append(-1)\r\n",
        "      i = i+1\r\n",
        "    \r\n",
        "\r\n",
        "  return img_a,img_b,img_y\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfFSjyNoBKNa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMZOMnpQBKP0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPKzPotl-5Db"
      },
      "source": [
        "\r\n",
        "aud_a = []\r\n",
        "aud_b = []\r\n",
        "aud_y = []\r\n",
        "\r\n",
        "def create_audio_data_pairs():\r\n",
        "\r\n",
        "  # our objective is to make half of the dataset witht the same classes and half with different classes.\r\n",
        "  i = 0\r\n",
        "\r\n",
        "  while(i<2500):\r\n",
        "\r\n",
        "    # perform pairing for same classes.\r\n",
        "    same_class = np.random.randint(N_CLASSES)\r\n",
        "    index = random.choice([k for k,j in enumerate(y) if j==same_class] , 2, replace=False)\r\n",
        "    x_random = X1[index]\r\n",
        "    y_same = same_class\r\n",
        "\r\n",
        "    aud_a.append(x_random[0])\r\n",
        "    aud_b.append(x_random[1])\r\n",
        "\r\n",
        "    aud_y.append(y_same)\r\n",
        "\r\n",
        "  i = 0\r\n",
        "\r\n",
        "  # different classes\r\n",
        "  while(i < 2500):\r\n",
        "\r\n",
        "    index = np.random.choice(X1.shape[0] , 2 ,replace = False)\r\n",
        "\r\n",
        "    x1_random = X1[index]\r\n",
        "    y1_random = y1[index]\r\n",
        "\r\n",
        "    # append the first audio to aud_a and second to aud_b\r\n",
        "    aud_a.append(x1_random[0])\r\n",
        "    aud_b.append(x1_random[1])\r\n",
        "\r\n",
        "    # finally based on the similarity of the audio images based on their classes append either the class (if they are similar) or -1 if they are dissimilar\r\n",
        "    if(y1_random[0] == y1_random[1]):\r\n",
        "      pass\r\n",
        "    else:\r\n",
        "      aud_y.append(-1)\r\n",
        "      i = i+1\r\n",
        "    \r\n",
        "\r\n",
        "    return aud_a , aud_b , aud_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aicLSWDRBJYd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cKkLcznBJh8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzpaj_2IQ9ti"
      },
      "source": [
        "# create the final data to be fed into the siamese network for training\r\n",
        "\r\n",
        "def multimodal_data_generate():\r\n",
        "  img_a , img_b , img_y = create_image_data_pairs()\r\n",
        "  aud_a , aud_b , aud_y = create_audio_data_pairs()\r\n",
        "\r\n",
        "  labels = []\r\n",
        "\r\n",
        "  for i in range(len(img_a)):\r\n",
        "    if(img_y==-1 or aud_y == -1):\r\n",
        "      labels.append(0)\r\n",
        "    elif(img_y==aud_y):\r\n",
        "      labels.append(1)\r\n",
        "    else:\r\n",
        "      labels.append(0)\r\n",
        "      \r\n",
        " return img_a,aud_a,img_b,aud_b,labels   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB8anpc6DbpC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbSuvq98BIKw"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\r\n",
        "base_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\r\n",
        "                                               include_top=False,\r\n",
        "                                               weights='imagenet')\r\n",
        "\r\n",
        "base_model.trainable = False\r\n",
        "base_model.summary()\r\n",
        "\r\n",
        "inputs = Input(shape=IMG_SHAPE)\r\n",
        "  x = inputs\r\n",
        "  x = base_model(x, training=False)\r\n",
        "  x = Flatten()(x)\r\n",
        "  x = Dense(128, activation='relu',kernel_regularizer='l2')(x)\r\n",
        "  x = Dropout(0.5)(x)\r\n",
        "  outputs = Dense(N_CLASSES , activation='softmax')(x)\r\n",
        "\r\n",
        "  model = Model(inputs,outputs)\r\n",
        "  model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zAdalujBIOW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onyt2y_fDcBI"
      },
      "source": [
        "# create the individual networks for image and audio to get the feature encodings\r\n",
        "def img_network(input_dim_img):\r\n",
        "  base_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\r\n",
        "                                               include_top=False,\r\n",
        "                                               weights='imagenet')\r\n",
        "\r\n",
        "  base_model.trainable = False\r\n",
        "  base_model.summary()\r\n",
        "\r\n",
        "  model = tf.keras.models.Sequential()\r\n",
        "  model.add(tf.keras.layers.Input(shape=IMG_SHAPE))\r\n",
        "  model.add(base_model)\r\n",
        "  model.add(tf.keras.layers.Flatten())\r\n",
        "  model.add(tf.keras.layers.Dense(128, activation='relu',kernel_regularizer='l2'))\r\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\r\n",
        "  model.summary()\r\n",
        "\r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def aud_network(input_dim_aud):\r\n",
        "\r\n",
        "  IMAGE_HEIGHT , IMAGE_WIDTH , N_CHANNELS = input_dim_aud # input shape\r\n",
        "\r\n",
        "  model = tf.keras.models.Sequential()\r\n",
        "  model.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\r\n",
        "  model.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'))\r\n",
        "  #model.add(tf.keras.layers.Dropout(0.005))\r\n",
        "  #model.add(tf.keras.layers.BatchNormalization())\r\n",
        "\r\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\n",
        "  #model.add(tf.keras.layers.BatchNormalization())\r\n",
        "  model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\r\n",
        "  #model.add(tf.keras.layers.BatchNormalization())\r\n",
        "  #model.add(tf.keras.layers.Dropout(0.01))\r\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\n",
        "  #model.add(tf.keras.layers.BatchNormalization())\r\n",
        "  model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\r\n",
        "  #model.add(tf.keras.layers.BatchNormalization())\r\n",
        "  #model.add(tf.keras.layers.Dropout(0.05))\r\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\n",
        "  #model.add(tf.keras.layers.BatchNormalization())\r\n",
        "  model.add(tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'))\r\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "  model.add(tf.keras.layers.Flatten())\r\n",
        "  model.add(tf.keras.layers.Dense(256, activation='relu'))\r\n",
        "  #model.add(tf.keras.layers.BatchNormalization())\r\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\r\n",
        "  model.add(tf.keras.layers.Dense(64, activation='relu'))\r\n",
        "  #model.add(tf.keras.layers.BatchNormalization())\r\n",
        "  model.add(tf.keras.layers.Dropout(0.25))\r\n",
        "\r\n",
        "  return model\r\n",
        "\r\n",
        "  \r\n",
        "def multimodal_network(input_dim):\r\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TXVWkDXDcGM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxdk5pPQ6jzT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDKhXzuv6j8D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWPPzFehDcOp"
      },
      "source": [
        "# siamese network model which helps in one-shot learning\r\n",
        "\r\n",
        "def siamese_network(input_dim_img,input_dim_aud):\r\n",
        "    \r\n",
        "    img_a = Input(shape=input_dim_img)\r\n",
        "    img_b = Input(shape=input_dim_img)\r\n",
        "    aud_a = Input(shape=input_dim_aud)\r\n",
        "    aud_b = Input(shape=input_dim_aud)\r\n",
        "\r\n",
        "    # extracting the image embeddings from the image network for two input images\r\n",
        "\r\n",
        "\r\n",
        "    img_network = image_feat_network(input_dim_img)\r\n",
        "\r\n",
        "    # img_network.load_weights()\r\n",
        "\r\n",
        "    feat_img_a = img_network(img_a)\r\n",
        "    feat_img_b = img_network(img_b)\r\n",
        "    \r\n",
        "    # extracting the audio embeddings from the image network for two output images\r\n",
        "\r\n",
        "    aud_network = audio_feat_network(input_dim_aud)\r\n",
        "\r\n",
        "    # aud_network.load_weights()\r\n",
        "\r\n",
        "    feat_aud_a = aud_network(aud_a)\r\n",
        "    feat_aud_b = aud_network(aud_b)\r\n",
        "    \r\n",
        "    # concatenate the first image's embeddings with the first audio's embeddings\r\n",
        "    concat_a = Concatenate(axis=3)([feat_img_a, feat_aud_a])\r\n",
        "    # concatenate the second image's embeddings with the second audio's embeddings\r\n",
        "    concat_b = Concatenate(axis=3)([feat_img_b, feat_aud_b])\r\n",
        "    \r\n",
        "    input_dim = concat_a.shape # (?) put the input dimensions here\r\n",
        "    base_network = multi_modal_network(input_dim)\r\n",
        "\r\n",
        "    # left vector encodings\r\n",
        "    encoded_l = base_network(concat_a) \r\n",
        "    # right vector encodings\r\n",
        "    encoded_r = base_network(concat_b)\r\n",
        "\r\n",
        "    #---->Layer to merge two encoded inputs with the l1 distance between them\r\n",
        "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\r\n",
        "\r\n",
        "    #---->Get the L1 Distance between the two encoded legs and predict a sigmoid using that.\r\n",
        "    L1_distance = L1_layer([encoded_l, encoded_r])\r\n",
        "    prediction = Dense(1,activation='sigmoid')(L1_distance)\r\n",
        "\r\n",
        "    model = Model(input=[img_a, aud_a, img_b,aud_b], output=prediction)\r\n",
        "\r\n",
        "    optimizer = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999)\r\n",
        "    #  model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\r\n",
        "\r\n",
        "    return optimizer,model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EPf4RkrDdkZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdlH9rVNDdrm"
      },
      "source": [
        "# final model train \r\n",
        "\r\n",
        "epochs = 30\r\n",
        "input_dim_img = (IMAGE_HEIGHT,IMAGE_WIDTH,N_CHANNELS)\r\n",
        "input_dim_aud = (AUD_HEIGHT,AUD_WIDTH,N_CHANNELS)\r\n",
        "\r\n",
        "img_a , aud_a , img_b , aud_b , labels = multimodal_data_generate()\r\n",
        "optimizer , model = siamese_network(input_dim_img , input_dim_aud)\r\n",
        "model.compile(loss = 'binary_crossentropy' , optimizer=optimizer)\r\n",
        "model.summary()\r\n",
        "\r\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\r\n",
        "model.fit([img_a,aud_a,img_b,aud_b], labels , validation_fit = 0.25 , batch_size = BATCH_SIZE , verbose = 2, nb_epochs = epochs , callbacks = [es] , metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIB-NYoRlVLh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kAqjQT8lVSs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1jRfsP6xoiZ"
      },
      "source": [
        "tf.reset_default_graph()\r\n",
        "faceModel = get_face_model()\r\n",
        "faceModel.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdTSBeWNkjib"
      },
      "source": [
        "# cp_callback = ModelCheckpoint('/content/gdrive/My Drive/Face Detection/training4',save_weights_only=True,verbose=1)\r\n",
        "\r\n",
        "weight_dir = \"/content/gdrive/My Drive/Face Detection/weight_face_detection_model\"\r\n",
        "if not os.path.exists(weight_dir):\r\n",
        "    os.mkdir(weight_dir)\r\n",
        "    \r\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=weight_dir+'/checkpoint-{epoch:02d}.hdf5',\r\n",
        "    save_weights_only=True,\r\n",
        "    monitor='val_accuracy',\r\n",
        "    mode='max',\r\n",
        "    save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hnq6WedknL6"
      },
      "source": [
        "if os.path.exists(weight_dir):\r\n",
        "  faceModel.load_weights(filepath=weight_dir+'/checkpoint-{epoch:02d}.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xW67jWfkoi3"
      },
      "source": [
        "# Train model for 10 epochs, capture the history\r\n",
        "history = faceModel.fit(train_dataset, epochs=10, validation_data=valid_dataset, callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf9fdtMsktax"
      },
      "source": [
        "faceModel.save('/content/gdrive/My Drive/models/face model2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4EamVpGkvi4"
      },
      "source": [
        "final_loss, final_acc = faceModel.evaluate(valid_dataset)\r\n",
        "print(\"Final loss: {0:.6f}, final accuracy: {1:.6f}\".format(final_loss, final_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwPcKdJ_kySV"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "acc = history.history['accuracy']\r\n",
        "val_acc = history.history['val_accuracy']\r\n",
        "\r\n",
        "loss = history.history['loss']\r\n",
        "val_loss = history.history['val_loss']\r\n",
        "\r\n",
        "plt.figure(figsize=(8, 8))\r\n",
        "plt.subplot(2, 1, 1)\r\n",
        "plt.plot(acc, label='Training Accuracy')\r\n",
        "plt.plot(val_acc, label='Validation Accuracy')\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.ylim([min(plt.ylim()),1])\r\n",
        "plt.title('Training and Validation Accuracy')\r\n",
        "\r\n",
        "plt.subplot(2, 1, 2)\r\n",
        "plt.plot(loss, label='Training Loss')\r\n",
        "plt.plot(val_loss, label='Validation Loss')\r\n",
        "plt.legend(loc='upper right')\r\n",
        "plt.ylabel('Cross Entropy')\r\n",
        "plt.ylim([0,1.0])\r\n",
        "plt.title('Training and Validation Loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}